{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_regularization",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sistelca/NOT-MNIST/blob/master/3_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VR6tIWUfNNS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABeZB9H2NdMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Deep Learning\n",
        "##Assignment 3\n",
        "Anteriormente, en 2_fullyconnected.ipynb, entrenó una regresión logística y un modelo de red neuronal.\n",
        "\n",
        "El objetivo de esta tarea es explorar técnicas de regularización."
      ]
    },
    {
      "metadata": {
        "id": "Umv1cjcsN-mE",
        "colab_type": "code",
        "outputId": "eede5491-a81a-4f76-8228-e653d5396e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6yCQM6jrOJ_P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Primero recarga los datos que generamos en 1_notmnist.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "gDHvtvurOLx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# corre solo para descomprimir la primera vez\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "filename = \"/content/gdrive/My Drive/curso_udacity/notMNIST.zip\"\n",
        "\n",
        "\t\n",
        "with ZipFile(filename) as zipObj:\n",
        "  # Extract all the contents of zip file in current directory\n",
        "  zipObj.extractall(\"/content/gdrive/My Drive/curso_udacity/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzkknLgfOlY3",
        "colab_type": "code",
        "outputId": "223547b4-1ada-42d4-c03f-f76ea3c10691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = '/content/gdrive/My Drive/curso_udacity//notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)\n",
        "  \n",
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0]) \n",
        "\n",
        "\n",
        "# Cambie el formato para adaptarlo a los modelos que vamos a entrenar:\n",
        "# datos como una matriz plana, Las etiquetas como float 1 codifican en caliente.\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 10\n",
        "batch_size = 128\n",
        "image_size = 28\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (50000, 28, 28) (50000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qa-zcRDUPOzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Problem 1\n",
        "Introduzca y ajuste la regularización de L2 para los modelos de redes neuronales y logísticas. Recuerde que L2 equivale a agregar una penalización en la norma de los pesos a la pérdida. En TensorFlow, puede calcular la pérdida de L2 para un tensor t utilizando nn.l2_loss (t). La cantidad correcta de regularización debería mejorar su validación / precisión de prueba."
      ]
    },
    {
      "metadata": {
        "id": "qGUkD4p6Pjw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# regularizacion L2 para logística\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default() as defa:\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)+0.01*tf.nn.l2_loss(weights))\n",
        "      \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2zd-e_S4B61T",
        "colab_type": "code",
        "outputId": "60edace3-66e6-4455-87c3-08bd29953ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"** Test presicion Global: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 46.579704\n",
            "Minibatch accuracy: 13.3%\n",
            "Validation accuracy: 18.0%\n",
            "Minibatch loss at step 500: 0.849907\n",
            "Minibatch accuracy: 80.5%\n",
            "Validation accuracy: 82.1%\n",
            "Minibatch loss at step 1000: 0.921997\n",
            "Minibatch accuracy: 75.8%\n",
            "Validation accuracy: 81.5%\n",
            "Minibatch loss at step 1500: 0.860147\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 82.1%\n",
            "Minibatch loss at step 2000: 0.666245\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 81.9%\n",
            "Minibatch loss at step 2500: 0.780673\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 82.2%\n",
            "Minibatch loss at step 3000: 0.714072\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 81.6%\n",
            "** Test presicion Global: 88.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_wqUoMnfC87o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# regularizacion para la red neuronal\n",
        "hidden_nodes = 1024 # neuronas en la primera capa oculta\n",
        "lamda_regula = .01\n",
        "tf_dropout_rate = 0.5\n",
        "  \n",
        "with graph.as_default() as gradef:\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                      shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "    # Variables\n",
        "    \n",
        "    # Capa 1\n",
        "    w_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
        "    b_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
        "    \n",
        "    # Capa 2\n",
        "    #w_2 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes//2]))\n",
        "    #b_2 = tf.Variable(tf.zeros([hidden_nodes//2]))\n",
        "    \n",
        "    # Capa 3 de salida\n",
        "    w_3 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
        "    b_3 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    def NN(x, tf_dropout_rate = tf_dropout_rate): # neurona\n",
        "        \"\"\"\n",
        "            x: matriz\n",
        "               su forma debe ser (m, 784)\n",
        "\n",
        "            regresa la activacion de la capa de salida\n",
        "            matriz de (m, 10)\n",
        "        \"\"\"\n",
        "\n",
        "        # Capa oculta 1\n",
        "        z_1 = tf.matmul(x,w_1) + b_1 # lineal\n",
        "        a_1 = tf.nn.relu(z_1)\t# activacion\n",
        "        # apply DropOut to hidden layer\n",
        "        drop_out = tf.nn.dropout(a_1, tf_dropout_rate)  # DROP-OUT here\n",
        "        \n",
        "        # Capa oculta 2\n",
        "#        z_2 = tf.matmul(drop_out1, w_2) + b_2\n",
        "#        a_2 = tf.nn.relu(z_2)\n",
        "#        drop_out = tf.nn.dropout(a_2, tf_dropout_rate)  # DROP-OUT here\n",
        "\n",
        "        # Capa 2 salida\n",
        "        z_3 = tf.matmul(drop_out,w_3) + b_3\n",
        "\n",
        "        return z_3\n",
        "    \n",
        "    # Training computation.\n",
        "    \n",
        "    y_ = NN(tf_train_dataset, tf_dropout_rate)  # dropout\n",
        "    \n",
        "    # funcion de costo\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=y_)+\n",
        "    lamda_regula*tf.nn.l2_loss(w_1)+\n",
        "    lamda_regula*tf.nn.l2_loss(w_2))\n",
        "    \n",
        "    # Optimizador.\n",
        "    optim = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "    \n",
        "    \n",
        "    # Predictions for the training, validation, and test data.\n",
        "    train_prediction = tf.nn.softmax(y_)\n",
        "    valid_prediction = tf.nn.softmax(NN(tf_valid_dataset, 1.0))\n",
        "    test_prediction = tf.nn.softmax(NN(tf_test_dataset, 1.0))\n",
        "    \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VASF3l_DOnk",
        "colab_type": "code",
        "outputId": "c84b68a3-e821-43f3-c3fe-cb71114c33d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optim, loss, y_], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "            valid_prediction.eval(), valid_labels))\n",
        "    print(\"Test presicion Global: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 5648.840820\n",
            "Minibatch accuracy: 7.0%\n",
            "Validation accuracy: 26.1%\n",
            "Minibatch loss at step 500: 40.619511\n",
            "Minibatch accuracy: 72.7%\n",
            "Validation accuracy: 79.9%\n",
            "Minibatch loss at step 1000: 1.382952\n",
            "Minibatch accuracy: 70.3%\n",
            "Validation accuracy: 82.4%\n",
            "Minibatch loss at step 1500: 0.904320\n",
            "Minibatch accuracy: 79.7%\n",
            "Validation accuracy: 83.1%\n",
            "Minibatch loss at step 2000: 0.728635\n",
            "Minibatch accuracy: 85.2%\n",
            "Validation accuracy: 82.8%\n",
            "Minibatch loss at step 2500: 0.779911\n",
            "Minibatch accuracy: 80.5%\n",
            "Validation accuracy: 83.8%\n",
            "Minibatch loss at step 3000: 0.702906\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 82.3%\n",
            "Test presicion Global: 88.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jAgFsie_P0z_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Problem 2 y 3\n",
        "- Vamos a demostrar un caso extremo de sobreeentrenamiento. Limite sus datos de entrenamiento a unos pocos lotes. ¿Qué es lo que pasa?\n",
        "\n",
        "- Introduce Dropout en la capa oculta de la red neuronal. Recuerda: La deserción sólo debe introducirse durante la capacitación, no durante la evaluación, de lo contrario, los resultados de la evaluación también serían estocásticos. TensorFlow proporciona nn.dropout() para ello, pero debes asegurarte de que sólo se inserta durante el entrenamiento.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "isHTNioDP9A_",
        "colab_type": "code",
        "outputId": "b8dd7702-cb10-47ff-b5e1-bc8ef5fe98b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# trabajaremos solo el caso de redes neuronales\n",
        "# se observa que ante una dramatica disminucion del numero de muestras\n",
        "# la regularizacion es necesaria para prevenir sobreentrenamiento\n",
        "# mas esto no mejora la presicion global del modelo;\n",
        "# en general cambiar los parametros de regularizacion no inside en \n",
        "# la precision resultante\n",
        "\n",
        "redneu(.01, 0.5)\n",
        "ejecute(train_dataset[0:1000, :], train_labels[0:1000, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 16109.240234\n",
            "Minibatch accuracy: 13.3%\n",
            "Validation accuracy: 31.4%\n",
            "Minibatch loss at step 500: 1.206977\n",
            "Minibatch accuracy: 80.5%\n",
            "Validation accuracy: 78.4%\n",
            "Minibatch loss at step 1000: 1.005678\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 80.5%\n",
            "Minibatch loss at step 1500: 1.111877\n",
            "Minibatch accuracy: 83.6%\n",
            "Validation accuracy: 79.5%\n",
            "Minibatch loss at step 2000: 0.925378\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 78.8%\n",
            "Minibatch loss at step 2500: 1.103470\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 80.0%\n",
            "Minibatch loss at step 3000: 1.169951\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 77.8%\n",
            "Test presicion Global: 84.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbmOxhJgQE2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFGbY5HUQFVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Problem 4\n",
        "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
        "\n",
        "One avenue you can explore is to add multiple layers.\n",
        "\n",
        "Another one is to use learning rate decay:\n",
        "\n",
        "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "\n",
        "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
        "\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"
      ]
    },
    {
      "metadata": {
        "id": "z5_LHSl8QvIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# al aumentar el numero de capas se redujo significativamente\n",
        "# la presicion del modelo\n",
        "# probamos ahora con learnid decay\n",
        "hidden_nodes = 1024 # neuronas en la primera capa oculta\n",
        "lamda_regula = .01\n",
        "tf_dropout_rate = 0.5\n",
        "  \n",
        "with graph.as_default() as gradef:\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                      shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "    # Variables\n",
        "    \n",
        "    global_step = tf.Variable(0)\n",
        "    \n",
        "    # Capa 1\n",
        "    w_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
        "    b_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
        "    \n",
        "    # Capa 2\n",
        "    #w_2 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes//2]))\n",
        "    #b_2 = tf.Variable(tf.zeros([hidden_nodes//2]))\n",
        "    \n",
        "    # Capa 3 de salida\n",
        "    w_3 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
        "    b_3 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    \n",
        "    def NN(x, tf_dropout_rate = tf_dropout_rate): # neurona\n",
        "        \"\"\"\n",
        "            x: matriz\n",
        "               su forma debe ser (m, 784)\n",
        "\n",
        "            regresa la activacion de la capa de salida\n",
        "            matriz de (m, 10)\n",
        "        \"\"\"\n",
        "\n",
        "        # Capa oculta 1\n",
        "        z_1 = tf.matmul(x,w_1) + b_1 # lineal\n",
        "        a_1 = tf.nn.relu(z_1)\t# activacion\n",
        "        # apply DropOut to hidden layer\n",
        "        drop_out = tf.nn.dropout(a_1, tf_dropout_rate)  # DROP-OUT here\n",
        "        \n",
        "        # Capa oculta 2\n",
        "#        z_2 = tf.matmul(drop_out, w_2) + b_2\n",
        "#        a_2 = tf.nn.relu(z_2)\n",
        "#        drop_out = tf.nn.dropout(a_2, tf_dropout_rate)  # DROP-OUT here\n",
        "\n",
        "        # Capa 2 salida\n",
        "        z_3 = tf.matmul(drop_out,w_3) + b_3\n",
        "\n",
        "        return z_3\n",
        "    \n",
        "    # Training computation.\n",
        "    \n",
        "    y_ = NN(tf_train_dataset, tf_dropout_rate)  # dropout\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(0.1, global_step, 100000, 0.6)\n",
        "    \n",
        "    # funcion de costo\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=y_)+\n",
        "    lamda_regula*tf.nn.l2_loss(w_1)+\n",
        "    lamda_regula*tf.nn.l2_loss(w_2))\n",
        "    \n",
        "    # Optimizador.\n",
        "    optim = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "    \n",
        "    \n",
        "    # Predictions for the training, validation, and test data.\n",
        "    train_prediction = tf.nn.softmax(y_)\n",
        "    valid_prediction = tf.nn.softmax(NN(tf_valid_dataset, 1.0))\n",
        "    test_prediction = tf.nn.softmax(NN(tf_test_dataset, 1.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esgucniKJZw9",
        "colab_type": "code",
        "outputId": "875832f8-83dd-406c-cb28-fc449b43c969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optim, loss, y_], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "            valid_prediction.eval(), valid_labels))\n",
        "    print(\"Test presicion Global: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 5612.298828\n",
            "Minibatch accuracy: 8.6%\n",
            "Validation accuracy: 28.9%\n",
            "Minibatch loss at step 500: 1902.742065\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 83.4%\n",
            "Minibatch loss at step 1000: 701.119751\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 84.1%\n",
            "Minibatch loss at step 1500: 257.295105\n",
            "Minibatch accuracy: 78.1%\n",
            "Validation accuracy: 84.5%\n",
            "Minibatch loss at step 2000: 94.139992\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 84.9%\n",
            "Minibatch loss at step 2500: 34.898041\n",
            "Minibatch accuracy: 85.2%\n",
            "Validation accuracy: 86.1%\n",
            "Minibatch loss at step 3000: 13.171215\n",
            "Minibatch accuracy: 85.2%\n",
            "Validation accuracy: 86.4%\n",
            "Test presicion Global: 92.4%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}